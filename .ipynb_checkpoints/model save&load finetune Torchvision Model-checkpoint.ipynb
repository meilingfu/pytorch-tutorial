{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.01, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [3008291748744, 3008291748888, 3008291748960, 3008291749032, 3008291749104, 3008291749176, 3008291749248, 3008291746872, 3008291749320, 3008291749392]}]\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass,self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,6,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,16*5*5)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "#initialize model\n",
    "model=TheModelClass()\n",
    "#initialize optimizer\n",
    "optimizer=optim.SGD(model.parameters(),lr=0.01,momentum=0.9)\n",
    "#print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "    \n",
    "#print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name,'\\t',optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 0.4.1\n",
      "Torchvision version: 0.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets,models,transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch version:\",torch.__version__)\n",
    "print(\"Torchvision version:\",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs\n",
    "\n",
    "data_dir=\"./hymenoptera_data\"\n",
    "#Models to choose from[ResNet,AlexNet,vgg,squeezenet,densenet,inception]\n",
    "model_name=\"squeezenet\"\n",
    "#number of classes in the dataset\n",
    "num_classes=2\n",
    "#batch size for training(change depending on how mang memories you have)\n",
    "batch_size=8\n",
    "#number of epochs to train for\n",
    "num_epochs=15\n",
    "#flags for finetune or feature extract\n",
    "feature_extract=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training and validation code\n",
    "#每个epoch后进行一次validation，根据validation准确率追踪最佳模型，每个epoch输出training和validation accuracy\n",
    "def train_model(model,dataloaders,criterion,optimizer,num_epochs=25,is_inception=False):\n",
    "    since=time.time()\n",
    "    val_acc_history=[]\n",
    "    best_model_wts=copy.deepcopy(model.state_dict())\n",
    "    best_acc=0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch,num_epochs-1))\n",
    "        print('-'*10)\n",
    "        #each epoch has training and validation phase\n",
    "        for phase in ['train','val']:\n",
    "            if phase=='train':\n",
    "                model.train() #set model to training mode\n",
    "            else:\n",
    "                model.eval() #set model to validation mode\n",
    "            running_loss=0.0\n",
    "            running_corrects=0\n",
    "            #Iterator over data\n",
    "            for inputs,labels in dataloaders[phase]:\n",
    "                inputs=inputs.to(device)\n",
    "                labels=labels.to(device)\n",
    "                #zero the parameter gradient\n",
    "                optimizer.zero_grad()\n",
    "                #forward,如果是训练模式，记录模型\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    #训练时考虑inception网络的附属输出，计算损失函数\n",
    "                    if is_inception and phase=='train':\n",
    "                        outputs,aux_outputs=model(inputs)\n",
    "                        loss1=criterion(outputs,labels)\n",
    "                        loss2=criterion(aux_outputs,labels)\n",
    "                        loss=loss1+0.4*loss2\n",
    "                    else:\n",
    "                        outputs=model(inputs)\n",
    "                        loss=criterion(outputs,labels)\n",
    "                    #预测的类别\n",
    "                    _,preds=torch.max(outputs,1)\n",
    "                    #训练：反向传播和更新参数\n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()      \n",
    "                #statistics\n",
    "                running_loss+=loss.item()*inputs.size(0)\n",
    "                running_corrects+=torch.sum(preds==labels.data)\n",
    "            epoch_loss=running_loss/len(dataloaders[phase].dataset)\n",
    "            epoch_acc=running_corrects.double()/len(dataloaders[phase].dataset)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase,epoch_loss,epoch_acc))\n",
    "            #deep copy the model\n",
    "            if phase=='val' and epoch_acc>best_acc:\n",
    "                best_model_wts=copy.deepcopy(model.state_dict())\n",
    "                best_acc=epoch_acc\n",
    "            if phase=='val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        print()\n",
    "    time_elapsed=time.time()-since\n",
    "    print('Train completed in {:.0f}m {:.0f}s'.format(time_elapsed//60,time_elapsed%60))\n",
    "    print('Best validation accuracy: {:.4f}'.format(best_acc))\n",
    "    #load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model,val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model parametes'requires_grad attribute\n",
    "#默认加载的预训练模型requires_grad属性为True\n",
    "def set_parameters_requires_grad(model,feature_extraction):\n",
    "    if feature_extraction:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13051\\Anaconda3\\lib\\site-packages\\torchvision\\models\\squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data)\n",
      "C:\\Users\\13051\\Anaconda3\\lib\\site-packages\\torchvision\\models\\squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, mean=0.0, std=0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace)\n",
      "    (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#CNN模型的最后一层通常是FC层，neural节点数目是数据集中的类别数目\n",
    "#initialize and reshape the network，修改网络最后一层\n",
    "def initialize_model(model_name,num_classes,feature_extract,use_pretrained=True):\n",
    "    model_ft=None\n",
    "    input_size=0   #输入图片的尺寸\n",
    "    \n",
    "    if model_name=='resnet':\n",
    "        model_ft=models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameters_requires_grad(model_ft,feature_extract)\n",
    "        num_ftrs=model_ft.fc.in_features\n",
    "        #重新定义最后一层全连接层\n",
    "        model_ft.fc=nn.Linear(num_ftrs,num_classes)\n",
    "        input_size=224\n",
    "    elif model_name=='alexnet':\n",
    "        model_ft=models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameters_requires_grad(model_ft,feature_extract)\n",
    "        num_ftrs=model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6]=nn.Linear(num_ftrs,num_classes)\n",
    "        input_size=224\n",
    "    elif model_name=='vgg':\n",
    "        model_ft=models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameters_features_grad(model_ft,feature_extract)\n",
    "        num_ftrs=model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6]=nn.Linear(num_ftrs,num_classes)\n",
    "        input_size=224\n",
    "    elif model_name=='squeezenet':\n",
    "        model_ft=models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameters_requires_grad(model_ft,feature_extract)\n",
    "        model_ft.classifier[1]=nn.Conv2d(512,num_classes,kernel_size=(1,1),stride=(1,1))\n",
    "        model_ft.num_classes=num_classes\n",
    "        input_size=224\n",
    "    elif model_name=='densenet':\n",
    "        model_ft=models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameters_requires_grad(model_ft,feature_extract)\n",
    "        num_ftrs=model_ft.classifier.in_features\n",
    "        model_ft.classifier=nn.Linear(num_ftrs,num_classes)\n",
    "        input_size=224\n",
    "    elif model_name=='inception':\n",
    "        model_ft=models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameters_requires_grad(model_ft,feature_extract)\n",
    "        #handle the auxilary net\n",
    "        num_ftrs=model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc=nn.Linear(num_ftrs,num_classes)\n",
    "        #hadle the primary net\n",
    "        num_ftrs=model_ft.fc.in_features\n",
    "        model_ft.fc=nn.Linear(num_ftrs,num_classes)\n",
    "        input_size=299\n",
    "    else:\n",
    "        print('Invalid model name.Exit...')\n",
    "        exit()\n",
    "    return model_ft,input_size\n",
    "#initialize model for this run\n",
    "model_ft,input_size=initialize_model(model_name,num_classes,feature_extract,use_pretrained=True)\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing datasets and dataloaders....\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Data augmentation and normalization for training\n",
    "data_transforms={\n",
    "    'train':transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val':transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])}\n",
    "print('Initializing datasets and dataloaders....')\n",
    "#create training and validation dataset\n",
    "image_datasets={x:datasets.ImageFolder(os.path.join(data_dir,x),data_transforms[x]) for x in ['train','val']}\n",
    "#create training and validation dataloader\n",
    "dataloaders_dict={x:torch.utils.data.DataLoader(image_datasets[x],batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "                      for x in ['train','val']}\n",
    "#detect if we have a GPU available\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "#send the model to GPU\n",
    "model_ft=model_ft.to(device)\n",
    "#将需要优化更新的参数聚集到一起，finetuning所有参数都要更新，feature extract只更新最后一层参数\n",
    "params_to_update=model_ft.parameters()\n",
    "print('Params to learn:')\n",
    "if feature_extract:\n",
    "    params_to_update=[]\n",
    "    for name,params in model_ft.named_parameters():\n",
    "        if params.requires_grad==True:\n",
    "            params_to_update.append(params)\n",
    "            print('\\t',name)\n",
    "else:\n",
    "    for name,params in model_ft.named_parameters():\n",
    "        if params.requires_grad==True:\n",
    "            print('\\t',name)\n",
    "#定义优化函数\n",
    "optimizer=optim.SGD(params_to_update,lr=0.001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.5861 Acc: 0.7172\n",
      "val Loss: 0.3776 Acc: 0.8627\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.2999 Acc: 0.8770\n",
      "val Loss: 0.3494 Acc: 0.8954\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.2492 Acc: 0.9057\n",
      "val Loss: 0.4257 Acc: 0.8431\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.2379 Acc: 0.8934\n",
      "val Loss: 0.3353 Acc: 0.9216\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.2282 Acc: 0.9098\n",
      "val Loss: 0.3425 Acc: 0.9150\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.1879 Acc: 0.9262\n",
      "val Loss: 0.3305 Acc: 0.9216\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.1625 Acc: 0.9180\n",
      "val Loss: 0.3490 Acc: 0.9150\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.1499 Acc: 0.9385\n",
      "val Loss: 0.3927 Acc: 0.9085\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.1648 Acc: 0.9180\n",
      "val Loss: 0.3852 Acc: 0.9085\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.1213 Acc: 0.9590\n",
      "val Loss: 0.3784 Acc: 0.9216\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.1101 Acc: 0.9590\n",
      "val Loss: 0.4004 Acc: 0.9085\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.1609 Acc: 0.9221\n",
      "val Loss: 0.3419 Acc: 0.9346\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.1491 Acc: 0.9303\n",
      "val Loss: 0.4116 Acc: 0.9216\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.1331 Acc: 0.9467\n",
      "val Loss: 0.4163 Acc: 0.9085\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.1150 Acc: 0.9631\n",
      "val Loss: 0.3580 Acc: 0.9346\n",
      "\n",
      "Train completed in 2m 18s\n",
      "Best validation accuracy: 0.9346\n"
     ]
    }
   ],
   "source": [
    "#Run training and validation step\n",
    "#Setup the loss function\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "#training and evaluation\n",
    "model_ft,hist=train_model(model_ft,dataloaders_dict,criterion,optimizer,num_epochs=num_epochs,is_inception=(model_name=='inception'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13051\\Anaconda3\\lib\\site-packages\\torchvision\\models\\squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data)\n",
      "C:\\Users\\13051\\Anaconda3\\lib\\site-packages\\torchvision\\models\\squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, mean=0.0, std=0.01)\n"
     ]
    }
   ],
   "source": [
    "#Comparison with model trained from scratch\n",
    "scratch_model,_=initialize_model(model_name,num_classes,feature_extract=False,use_pretrained=False)\n",
    "scratch_model=scratch_model.to(device)\n",
    "scratch_criterion=nn.CrossEntropyLoss()\n",
    "scratch_optimizer=optim.SGD(scratch_model.parameters(),lr=0.001,momentum=0.9)\n",
    "_,scratch_hist=train_model(scratch_model,dataloaders_dict,scratch_criterion,scratch_optimizer,\n",
    "                           num_epochs=num_epochs,is_inception=(model_name=='inception'))\n",
    "#绘制validation accuracy随epoch的曲线\n",
    "ohist=[]\n",
    "shist=[]\n",
    "ohist=[h.cpu().numpy() for h in hist]\n",
    "shist=[h.cpu().numpy() for h in scratch_hist]\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1,num_epochs+1,1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
